{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/different_types_gans/SwinIR_wrapper/SwinIR_wrapper/SwinIR.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# #@title Imports and Utils { display-mode: \"form\" }\n",
    "\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "# %config InlineBackend.rc = {'figure.figsize': (10.0, 10.0)}\n",
    "\n",
    "# !git clone -qq https://github.com/Lin-Sinorodin/SwinIR_wrapper.git\n",
    "# !pip install -qq timm\n",
    "from SwinIR_wrapper.SwinIR_wrapper import SwinIR_SR\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda: 1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'Using GPU: {torch.cuda.get_device_properties(0).name}')\n",
    "else:\n",
    "    print('Using CPU. Concider using GPU for faster inference.')\n",
    "\n",
    "def compare_sr_with_original(img_lq, img_hq):\n",
    "    plt.figure()\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_lq[::,::,::-1])\n",
    "    plt.title(f'Original - {img_lq.shape}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img_hq[::,::,::-1])\n",
    "    plt.title(f'Super Resolution - {img_hq.shape}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title Get Test Image { run: \"auto\", display-mode: \"form\" }\n",
    "\n",
    "# #@markdown > #### Choose between uploading image or downloading from url:\n",
    "# get_image_method = \"download from url\" #@param [\"upload file\", \"download from url\"]\n",
    "\n",
    "# #@markdown * for uploaded image, write it's path:\n",
    "# uploaded_file = \"/content/my_uploaded_image.jpeg\" #@param {type:\"string\"}\n",
    "# if get_image_method == 'upload file':\n",
    "#     path = uploaded_file\n",
    "\n",
    "# #@markdown * for url download, provide the image url:\n",
    "# url = 'https://map.pepperdine.edu/maps/UMAP_2011012422072_6_8_19.jpg' #@param {type:\"string\"}\n",
    "# if get_image_method == 'download from url':\n",
    "#     path = url.split('/')[-1]\n",
    "#     urllib.request.urlretrieve(url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/bihar_most_15/images/9744500_2967894.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/different_types_gans/SwinIR_wrapper/SwinIR_wrapper/SwinIR.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_weights = torch.load(weights_path)\n",
      "/home/rishabh.mondal/.local/lib/python3.12/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded real_sr x4 successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#@title Setup Super Resolution Model { run: \"auto\" }\n",
    "pretrained_model = \"real_sr x4\" #@param [\"real_sr x4\", \"classical_sr x2\", \"classical_sr x3\", \"classical_sr x4\", \"classical_sr x8\", \"lightweight x2\", \"lightweight x3\", \"lightweight x4\"]\n",
    "\n",
    "model_type, scale = pretrained_model.split(' ')\n",
    "scale = int(scale[1])\n",
    "\n",
    "# initialize super resolution model\n",
    "sr = SwinIR_SR(model_type, scale)\n",
    "\n",
    "print(f'Loaded {pretrained_model} successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@12.274] global loadsave.cpp:241 findDecoder imread_('/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/vlm_data/bihar_most_15/images/9744500_2967894.tif'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m img_lq \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(path, cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# feed the image to the SR model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m img_hq \u001b[38;5;241m=\u001b[39m \u001b[43msr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_lq\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      6\u001b[0m image_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img_hq, (img_lq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], img_lq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), interpolation \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mINTER_AREA)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# save the result\u001b[39;00m\n",
      "File \u001b[0;32m~/Brick-Kilns-project/ijcai_2025_kilns/different_types_gans/SwinIR_wrapper/SwinIR_wrapper/SwinIR.py:113\u001b[0m, in \u001b[0;36mSwinIR_SR.upscale\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupscale\u001b[39m(\u001b[38;5;28mself\u001b[39m, img: np\u001b[38;5;241m.\u001b[39marray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39marray:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"feed the given image to the super resolution model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     h_in, w_in, _ \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m    114\u001b[0m     h_out, w_out \u001b[38;5;241m=\u001b[39m h_in \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, w_in \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# load the test image from previus step\n",
    "img_lq = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "\n",
    "# feed the image to the SR model\n",
    "img_hq = sr.upscale(img_lq)    \n",
    "image_resized = cv2.resize(img_hq, (img_lq.shape[1], img_lq.shape[0]), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "# save the result\n",
    "cv2.imwrite(f'{pretrained_model}.jpg', image_resized)\n",
    "# show results\n",
    "compare_sr_with_original(img_lq, image_resized) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    " # Replace with the actual module or model you're using\n",
    "\n",
    "# Paths\n",
    "input_dir = '../data/region_performance/bihar_same_class_count_10_120_1000/images'\n",
    "output_dir = '../data/swinir_data/bihar_same_class_count_10_120_1000/images'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to process images\n",
    "def process_images(input_dir, output_dir):\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.tif'):  # Check for .tif files\n",
    "            # Load the low-quality image\n",
    "            img_lq_path = os.path.join(input_dir, filename)\n",
    "            img_lq = cv2.imread(img_lq_path, cv2.IMREAD_COLOR)\n",
    "            \n",
    "            # Apply the super-resolution model\n",
    "            img_hq = sr.upscale(img_lq)  # Adjust the method based on your SR model's API\n",
    "            \n",
    "            # Resize the image back to the original dimensions\n",
    "            image_resized = cv2.resize(img_hq, (img_lq.shape[1], img_lq.shape[0]), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            # Save the result as a .png file\n",
    "            output_filename = os.path.splitext(filename)[0] + '.png'\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            cv2.imwrite(output_path, image_resized)\n",
    "            \n",
    "            # Optional: Display or compare the images (for debugging or validation)\n",
    "            # compare_sr_with_original(img_lq, image_resized)  # Implement or call your comparison function\n",
    "\n",
    "# Execute the function\n",
    "process_images(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
